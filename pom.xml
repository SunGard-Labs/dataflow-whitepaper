<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <groupId>com.sungard.dataflow</groupId>
  <artifactId>options-transform</artifactId>
  <version>0.0.1-SNAPSHOT</version>
  <packaging>jar</packaging>
  <name>options-data-transform</name>
  <url>https://github.com/SunGard-Labs/dataflow-whitepaper</url>
  <properties>
    <timestamp>${maven.build.timestamp}</timestamp>
    <maven.build.timestamp.format>yyyy-MM-dd-HH-mm</maven.build.timestamp.format>
    <gcp.project>sungard-cat-demo</gcp.project>
    <gcp.stagingLocation>gs://dataflow-staging-test</gcp.stagingLocation>
    <df.jobName>options-symbol-flow-${timestamp}</df.jobName>
    <df.jobFile>/tmp/dataflow-job-file</df.jobFile>
    <df.workerMachineType>n1-standard-4</df.workerMachineType>
    <df.numWorkers>20</df.numWorkers>
    <df.workerDiskSize>50</df.workerDiskSize>
    <df.bqDest>dataflowpaper.optionsdata</df.bqDest>
    <df.output>${gcp.project}:${df.bqDest}</df.output>
    <df.scalingAlgorithm>BASIC</df.scalingAlgorithm>
    <df.input>${basedir}/input/zvzzt.input.txt</df.input>
	<!--	<df.input>gs://dataflow-staging-test/zvzzt.input.txt</df.input> -->
	<df.output>${basedir}/output/zvzzt.output.txt</df.output>
	<!--    <df.output>${df.bqDest}</df.output> -->
    <slf4j.version>1.7.7</slf4j.version>
    <logback-classic.version>1.0.1</logback-classic.version>
    <junit.version>4.10</junit.version>
    <hamcrest.version>1.3</hamcrest.version>
    <mockito-all.version>1.10.19</mockito-all.version>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <maven.compiler.source>1.7</maven.compiler.source>
    <maven.compiler.target>1.7</maven.compiler.target>
  </properties>
  <dependencyManagement>
    <dependencies>
      <dependency>
        <groupId>com.google.cloud.dataflow</groupId>
        <artifactId>google-cloud-dataflow-java-sdk-all</artifactId>
        <version>LATEST</version>
      </dependency>
      <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-api</artifactId>
        <version>${slf4j.version}</version>
      </dependency>
      <dependency>
        <groupId>ch.qos.logback</groupId>
        <artifactId>logback-classic</artifactId>
        <version>${logback-classic.version}</version>
      </dependency>
      <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>${junit.version}</version>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.mockito</groupId>
        <artifactId>mockito-all</artifactId>
        <version>${mockito-all.version}</version>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.hamcrest</groupId>
        <artifactId>hamcrest-library</artifactId>
        <version>${hamcrest.version}</version>
        <scope>test</scope>
      </dependency>
    </dependencies>
  </dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.google.guava</groupId>
      <artifactId>guava</artifactId>
      <version>18.0</version>
    </dependency>
    <dependency>
      <groupId>com.google.cloud.dataflow</groupId>
      <artifactId>google-cloud-dataflow-java-sdk-all</artifactId>
    </dependency>
    <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-api</artifactId>
    </dependency>
    <dependency>
      <groupId>ch.qos.logback</groupId>
      <artifactId>logback-classic</artifactId>
    </dependency>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
    </dependency>
    <dependency>
      <groupId>org.mockito</groupId>
      <artifactId>mockito-all</artifactId>
    </dependency>
    <dependency>
      <groupId>org.hamcrest</groupId>
      <artifactId>hamcrest-library</artifactId>
    </dependency>
  </dependencies>
  <build>
    <plugins>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-shade-plugin</artifactId>
        <version>1.6</version>
        <configuration>
          <createDependencyReducedPom>true</createDependencyReducedPom>
          <filters>
            <filter>
              <artifact>*:*</artifact>
              <excludes>
                <exclude>META-INF/*.SF</exclude>
                <exclude>META-INF/*.DSA</exclude>
                <exclude>META-INF/*.RSA</exclude>
              </excludes>
            </filter>
          </filters>
        </configuration>
        <executions>
          <execution>
            <phase>package</phase>
            <goals>
              <goal>shade</goal>
            </goals>
            <configuration>
              <artifactSet>
                <!-- include any jar that would normally be in the containers lib -->
                <excludes>
                  <!-- <exclude>org.apache.storm:*</exclude> -->
                  <exclude>io.netty:*</exclude>
                  <exclude>org.objenesis:*</exclude>
                  <exclude>com.esotericsoftware.reflectasm:reflectasm</exclude>
                  <exclude>com.esotericsoftware:reflectasm</exclude>
                  <exclude>com.esotericsoftware.minlog:minlog</exclude>
                  <exclude>org.mortbay.jetty:*</exclude>
                </excludes>
              </artifactSet>
              <transformers>
                <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer" />
                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                  <mainClass>com.sungard.dataflow.SymbolTransformPipeline</mainClass>
                </transformer>
              </transformers>
            </configuration>
          </execution>
        </executions>
      </plugin>
    </plugins>
  </build>
  <profiles>
    <profile>
      <id>local</id>
      <build>
        <plugins>
          <plugin>
            <groupId>org.codehaus.mojo</groupId>
            <artifactId>exec-maven-plugin</artifactId>
            <version>1.2</version>
            <configuration>
              <executable>java</executable>
              <arguments>
                <argument>-classpath</argument>
                <classpath />
                <argument>com.sungard.dataflow.SymbolTransformPipeline</argument>
                <argument>--runner=DirectPipelineRunner</argument>
                <argument>--project=${gcp.project}</argument>
                <argument>--stagingLocation=${gcp.stagingLocation}</argument>
                <argument>--jobName=${df.jobName}</argument>
                <argument>--dataflowJobFile={df.jobFile}</argument>
                <argument>--autoscalingAlgorithm=${df.scalingAlgorithm}</argument>
                <argument>--workerMachineType=${df.workerMachineType}</argument>
                <argument>--diskSizeGb=${df.workerDiskSize}</argument>
                <argument>--numWorkers=${df.numWorkers}</argument>
                <argument>--inputFilePath=${df.input}</argument>
                <argument>--outputFilePath=${df.output}</argument>
              </arguments>
            </configuration>
          </plugin>
        </plugins>
      </build>
    </profile>
    <profile>
      <id>gcp</id>
      <build>
        <plugins>
          <plugin>
            <groupId>org.codehaus.mojo</groupId>
            <artifactId>exec-maven-plugin</artifactId>
            <version>1.2</version>
            <configuration>
              <executable>java</executable>
              <arguments>
                <argument>-classpath</argument>
                <classpath />
                <argument>com.sungard.dataflow.SymbolTransformPipeline</argument>
                <argument>--runner=BlockingDataflowPipelineRunner</argument>
                <argument>--project=${gcp.project}</argument>
                <argument>--stagingLocation=${gcp.stagingLocation}</argument>
                <argument>--jobName=${df.jobName}</argument>
                <argument>--dataflowJobFile={df.jobFile}</argument>
                <argument>--autoscalingAlgorithm=${df.scalingAlgorithm}</argument>
                <argument>--workerMachineType=${df.workerMachineType}</argument>
                <argument>--diskSizeGb=${df.workerDiskSize}</argument>
                <argument>--numWorkers=${df.numWorkers}</argument>
                <argument>--inputFilePath=${df.input}</argument>
                <argument>--outputTable=${df.output}</argument>
              </arguments>
            </configuration>
          </plugin>
        </plugins>
      </build>
    </profile>
  </profiles>
</project>
